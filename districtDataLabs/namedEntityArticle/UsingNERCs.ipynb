{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Named Entity Recognition and Classifiers to Extract Entities from Peer-Reviewed Journals\n",
    "\n",
    "The overwhelming amount of unstructured text data available today from traditional media sources as well as newer ones, like social media, provides a rich source of information if the data can be structured.  Named entity extraction forms a core subtask to build knowledge from semi-structured and unstructured text sources<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>.  Some of the first researchers working to extract information from unstructured texts recognized the importance of “units of information” like names, including person, organization and location names, and numeric expressions including time, date, money and percent expressions.  They coined the term “Named Entity” in 1996 to represent these. Considering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them. These knowledge bases are key contributors to intelligent computer behavior<sup><a href=\"#fn2\" id=\"ref2\">2</a></sup>.  Not surprisingly, named entity extraction operates at the core of several popular technologies such as smart assistants ([Siri](http://www.apple.com/ios/siri/), [Google Now](https://www.google.com/landing/now/)), machine reading, and deep interpretation of natural language<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup>.\n",
    "\n",
    "This post explores how to perform named entity extraction, formally known as “[Named Entity Recognition and Classification (NERC)](https://benjamins.com/catalog/bct.19).  In addition, the article surveys open-source NERC tools that work with Python and compares the results obtained using them against hand-labeled data. The specific steps include: preparing semi-structured natural language data for ingestion using regular expressions; creating a custom corpus in the [Natural Language Toolkit](http://www.nltk.org/); using a suite of open source NERC tools to extract entities and store them in JSON format; comparing the performance of the NERC tools, and implementing a simplistic ensemble classifier. The information extraction concepts and tools in this article constitute a first step in the overall process of structuring unstructured data.  They can be used to perform more complex natural language processing to derive unique insights from large collections of unstructured data.\n",
    "<br>\n",
    "# Environment Set Up\n",
    "\n",
    "To recreate the work in this article, use [Anaconda](https://www.continuum.io/why-anaconda), which is an easy-to-install, free, enterprise-ready Python distribution for data analytics, processing, and scientific computing (reference). With a few lines of code, you can have all the dependencies used in this post with the exception of one function (email extractor). \n",
    "\n",
    "*  [Install Anaconda](http://docs.continuum.io/anaconda/install)\n",
    "\n",
    "*  Download the [namedentity_requirements.yml](https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/namedentity_requirements.yml) (remember where you saved it on your computer)\n",
    "*  Follow the [\"Use Environment from file\"](http://conda.pydata.org/docs/using/envs.html#use-environment-from-file) instructions on Anaconda's website.\n",
    "\n",
    "If you use an alternative method to set up a virtual environment, make sure you have all the files installed from the yml file. The one dependency not in the yml file is the email extractor. [Cut and paste the function from this website](https://gist.github.com/dideler/5219706), save it to a .py file, and make sure it is in your sys.path or environment path.  If you are [running this as an iPython notebook](https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/blogdraft.ipynb), stop here. Go to the Appendix and run all of the blocks of code before continuing.\n",
    "\n",
    "\n",
    "### Data Source\n",
    "The proceedings from the Knowledge Discovery and Data Mining (KDD) conferences in [New York City (2014)](http://www.kdd.org/kdd2014/) and [Sydney, Australia (2015)](http://www.kdd.org/kdd2015/) serve as our source of unstructured text and contain over 230 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science and their applications. The full conference proceedings can be purchased for $60 at the [Association for Computing Machinery's Digital Library](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585) (includes ACM membership). This post will work with a dataset that is equivalent to the combined conference proceedings and takes the semi-structured data that is in the form of PDF journal articles and abstracts, extracts text from these files, and adds structure to the data to facilitate follow-on analysis.  Interested parties looking for a free option can use the [`beautifulsoup`](https://pypi.python.org/pypi/beautifulsoup4/4.4.1) and [`requests`](https://pypi.python.org/pypi/requests/2.9.1)libraries to [scrape the ACM website for KDD 2015 conference data](https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb).\n",
    "\n",
    "### Initial Data Exploration\n",
    "Visual inspection reveals that the target filenames begin with a “p” and end with “pdf.” As a first step, we determine the number of files and the naming conventions by using a loop to iterate over the files in the directory and printing out the filenames.  Each filename also gets saved to a list, and the length of the list tells us the total number of files in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "\n",
      "[p1.pdf, p1005.pdf, p1015.pdf, p1025.pdf, p1035.pdf, p1045.pdf, p1055.pdf, p1065.pdf, p1075.pdf, p1085.pdf, p109.pdf, p1095.pdf, p1105.pdf, p1115.pdf, p1125.pdf, p1135.pdf, p1145.pdf, p1155.pdf, p1165.pdf, p1175.pdf, p1185.pdf, p119.pdf, p1195.pdf, p1205.pdf, p1215.pdf, p1225.pdf, p1235.pdf, p1245.pdf, p1255.pdf, p1265.pdf, p1275.pdf, p1285.pdf, p129.pdf, p1295.pdf, p1305.pdf, p1315.pdf, p1325.pdf, p1335.pdf, p1345.pdf, p1355.pdf, p1365.pdf, p1375.pdf, p1385.pdf, p139.pdf, p1395.pdf, p1405.pdf, p1415.pdf, p1425.pdf, p1435.pdf, p1445.pdf, p1455.pdf, p1465.pdf, p1475.pdf, p1485.pdf, p149.pdf, p1495.pdf, p1503.pdf, p1513.pdf, p1523.pdf, p1533.pdf, p1543.pdf, p1553.pdf, p1563.pdf, p1573.pdf, p1583.pdf, p159.pdf, p1593.pdf, p1603.pdf, p1621.pdf, p1623.pdf, p1625.pdf, p1627.pdf, p1629.pdf, p1631.pdf, p1633.pdf, p1635.pdf, p1637.pdf, p1639.pdf, p1641.pdf, p1651.pdf, p1661.pdf, p1671.pdf, p1681.pdf, p169.pdf, p1691.pdf, p1701.pdf, p1711.pdf, p1721.pdf, p1731.pdf, p1741.pdf, p1751.pdf, p1759.pdf, p1769.pdf, p1779.pdf, p1789.pdf, p179.pdf, p1799.pdf, p1809.pdf, p1819.pdf, p1829.pdf, p1839.pdf, p1849.pdf, p1859.pdf, p1869.pdf, p1879.pdf, p1889.pdf, p189.pdf, p1899.pdf, p19.pdf, p1909.pdf, p1919.pdf, p1929.pdf, p1939.pdf, p1949.pdf, p1959.pdf, p1969.pdf, p1979.pdf, p1989.pdf, p199.pdf, p1999.pdf, p2009.pdf, p2019.pdf, p2029.pdf, p2039.pdf, p2049.pdf, p2059.pdf, p2069.pdf, p2079.pdf, p2089.pdf, p209.pdf, p2099.pdf, p2109.pdf, p2119.pdf, p2127.pdf, p2137.pdf, p2147.pdf, p2157.pdf, p2167.pdf, p2177.pdf, p2187.pdf, p219.pdf, p2197.pdf, p2207.pdf, p2217.pdf, p2227.pdf, p2237.pdf, p2247.pdf, p2257.pdf, p2267.pdf, p2277.pdf, p2287.pdf, p229.pdf, p2297.pdf, p2307.pdf, p2309.pdf, p2311.pdf, p2313.pdf, p2315.pdf, p2317.pdf, p2319.pdf, p2321.pdf, p2323.pdf, p2325.pdf, p2327.pdf, p2329.pdf, p239.pdf, p249.pdf, p259.pdf, p269.pdf, p279.pdf, p289.pdf, p29.pdf, p299.pdf, p3.pdf, p309.pdf, p319.pdf, p329.pdf, p339.pdf, p349.pdf, p359.pdf, p369.pdf, p379.pdf, p387.pdf, p39.pdf, p397.pdf, p407.pdf, p417.pdf, p427.pdf, p437.pdf, p447.pdf, p457.pdf, p467.pdf, p477.pdf, p487.pdf, p49.pdf, p497.pdf, p5.pdf, p507.pdf, p517.pdf, p527.pdf, p537.pdf, p547.pdf, p557.pdf, p567.pdf, p577.pdf, p587.pdf, p59.pdf, p597.pdf, p607.pdf, p617.pdf, p627.pdf, p635.pdf, p645.pdf, p655.pdf, p665.pdf, p675.pdf, p685.pdf, p69.pdf, p695.pdf, p7.pdf, p705.pdf, p715.pdf, p725.pdf, p735.pdf, p745.pdf, p755.pdf, p765.pdf, p775.pdf, p785.pdf, p79.pdf, p805.pdf, p815.pdf, p825.pdf, p835.pdf, p845.pdf, p855.pdf, p865.pdf, p875.pdf, p885.pdf, p89.pdf, p895.pdf, p9.pdf, p905.pdf, p915.pdf, p925.pdf, p935.pdf, p945.pdf, p955.pdf, p965.pdf, p975.pdf, p985.pdf, p99.pdf, p995.pdf]\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# Administrative code: Import what we need\n",
    "##############################################\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "###############################################\n",
    "# Set the Path\n",
    "##############################################\n",
    "\n",
    "path        = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Path to directory where KDD files are\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"Desktop\",\"KDD_15\",\"docs\"))\n",
    "\n",
    "# Establish an empty list to append filenames as we iterate over the directory with filenames\n",
    "files = []\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Code to iterate over files in directory\n",
    "##############################################\n",
    "\n",
    "'''Iterate over the directory of filenames and add to list.  \n",
    "Inspection shows our target filenames begin with 'p' and end with 'pdf' '''\n",
    "\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            files.append(fileName)\n",
    "end_time = time.time()\n",
    "\n",
    "###############################################\n",
    "# Output\n",
    "###############################################print\n",
    "print len(files) # Print the number of files\n",
    "print \n",
    "print '[%s]' % ', '.join(map(str, files)) # print the list of filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>A total of 253 files exist in the directory. Opening one of these reveals that our data is in PDF format and it's semi-structured (follows journal article format with separate sections for \"abstract\" and \"title\"). While PDFs provide an easily readable presentation of data, they are extremely difficult to work with in data analysis. In your work, if you have an option to get to data before conversion to a PDF format, be sure to take that option.<br><br>\n",
    "\n",
    "### Creating a Custom NLTK Corpus\n",
    "\n",
    "We used several Python tools to ingest our data including: [`pdfminer`](https://pypi.python.org/pypi/pdfminer/), [`subprocess`](https://docs.python.org/2/library/subprocess.html),  [`nltk`](http://www.nltk.org/), [`string`](https://docs.python.org/2/library/string.html), and [`unicodedata`](https://docs.python.org/2/library/unicodedata.html).  Pdfminer contains a command line tool called “pdf2txt.py” that extracts text contents from a PDF file (visit the [`pdfminer homepage`](http://euske.github.io/pdfminer/index.html#pdf2txt) for download instructions).  Subprocess, a standard library module, allows us to invoke the “pdf2txt.py” command line tool within our code.  The Natural Language Tool Kit, or NLTK, serves as one of Python’s leading platforms to analyze natural language data.  The string module provides variable substitutions and value formatting to strip non-printable characters from the output of the text extracted from our journal article PDFs.  Finally, the unicodedata library allows Latin Unicode characters to degrade gracefully into ASCII.  This is an important feature because some Unicode characters won’t extract nicely.\n",
    "\n",
    "Our task begins by iterating over the files in the directory with names that begin with 'p' and end with 'pdf.' This time, however, we will strip the text from the pdf file, write the .txt file to a newly created directory, and use the fileName variable to name the files we write to disk. Keep in mind that this task may take a few minutes depending on the processing power of your computer. Next, we use the simple instructions from Section 1.9, Chapter 2 of NLTK's Book to build a custom corpus. Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals. Here's the code to accomplish what's discussed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Importing what we need\n",
    "###############################################\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import nltk\n",
    "import os, os.path\n",
    "import re\n",
    "\n",
    "###############################################\n",
    "# Create the directory we will write the .txt files to after stripping text\n",
    "###############################################\n",
    "\n",
    "# path where KDD journal files exist on disk or cloud drive access\n",
    "corpuspath = os.path.normpath(os.path.expanduser('~/Desktop/KDD_corpus/'))\n",
    "if not os.path.exists(corpuspath):\n",
    "    os.mkdir(corpuspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############################################\n",
    "# Core code to iterate over files in the directory\n",
    "###############################################\n",
    "\n",
    "# We start from the code to iterate over the files\n",
    "%timeit\n",
    "\n",
    "for dirName, subdirList, fileList in os.walk(TESTDIR):\n",
    "    for fileName in fileList:\n",
    "        if fileName.startswith('p') and fileName.endswith('.pdf'):\n",
    "            if os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                pass\n",
    "            else:\n",
    "            \n",
    "            \n",
    "###############################################\n",
    "# This code strips the text from the PDFs\n",
    "###############################################\n",
    "                try:\n",
    "                    document = filter(lambda x: x in string.printable,\n",
    "                                      unicodedata.normalize('NFKD', \n",
    "                                                            (unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),\n",
    "                                                                     errors='ignore'))).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))\n",
    "                except UnicodeDecodeError:\n",
    "                    document = unicodedata.normalize('NFKD',\n",
    "                                                     unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,fileName)))]),errors='ignore')).encode('ascii','ignore')    \n",
    "\n",
    "                if len(document)<300:\n",
    "                    pass\n",
    "                else:\n",
    "                    # used this for assistance http://stackoverflow.com/questions/2967194/open-in-python-does-not-create-a-file-if-it-doesnt-exist\n",
    "                    if not os.path.exists(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\"))):\n",
    "                        file = open(os.path.normpath(os.path.join(corpuspath,fileName.split(\".\")[0]+\".txt\")), 'w+')\n",
    "                        file.write(document)\n",
    "                    else:\n",
    "                        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code builds our custom corpus.  The corpus path is a path to where we saved all of our .txt files of stripped text                    \n",
    "kddcorpus= nltk.corpus.PlaintextCorpusReader(corpuspath, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We now have a semi-structured dataset in a format that we can query and analyze the different pieces of data. Let's see how many words (including stop words) we have in our entire corpus. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795267\n"
     ]
    }
   ],
   "source": [
    "# Mapping, setting count to zero for start\n",
    "wordcount = 0\n",
    "\n",
    "#Iterating over list and files and counting length\n",
    "for fileid in kddcorpus.fileids():\n",
    "    wordcount += len(kddcorpus.words(fileid))\n",
    "print wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>The NLTK book has an excellent [section on processing raw text and unicode issues](http://www.nltk.org/book/ch03.html#fig-unicode). It provides a helpful discussion of some problems you may encounter. \n",
    "\n",
    "###Using Regular Expressions to Extract Specific Sections \n",
    "\n",
    "<br>\n",
    "To begin our exploration of regular expressions (aka \"regex\"), it's important to point out some good resources for those new to the topic. An excellent resource may be found in [Videos 1-3, Week 4, Getting and Cleaning Data, Data Science Specialization Track from Johns Hopkins University](https://www.coursera.org/learn/data-cleaning).   Additional resources appear in the Appendix.  As a simple example, let’s extract titles from the first 26 documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online Controlled Experiments: Lessons from Running A/B/n Tests for 12 Years\n",
      "Mining Frequent Itemsets through Progressive Sampling with Rademacher Averages\n",
      "Why It Happened: Identifying and Modeling the Reasons of the Happening of Social Events\n",
      "Matrix Completion with Queries Natali Ruchansky\n",
      "Stochastic Divergence Minimization for Online Collapsed Variational Bayes Zero Inference\n",
      "Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts\n",
      "TimeCrunch: Interpretable Dynamic Graph Summarization Neil Shah\n",
      "Inside Jokes: Identifying Humorous Cartoon Captions Dafna Shahaf\n",
      "Community Detection based on Distance Dynamics Junming Shao\n",
      "Discovery of Meaningful Rules in Time Series Mohammad Shokoohi-Yekta    Yanping Chen    Bilson Campana    Bing Hu\n",
      "On the Formation of Circles in Co-authorship Networks Tanmoy Chakraborty1, Sikhar Patranabis2, Pawan Goyal3, Animesh Mukherjee4\n",
      "An Evaluation of Parallel Eccentricity Estimation Algorithms on Undirected Real-World Graphs\n",
      "Efcient Latent Link Recommendation in Signed Networks\n",
      "Turn Waste into Wealth: On Simultaneous Clustering and Cleaning over Dirty Data\n",
      "Set Cover at Web Scale Stergios Stergiou\n",
      "Exploiting Relevance Feedback in Knowledge Graph Search\n",
      "LINKAGE: An Approach for Comprehensive Risk Prediction for Care Management\n",
      "Transitive Transfer Learning Ben Tan\n",
      "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks\n",
      "An Effective Marketing Strategy for Revenue Maximization with a Quantity Constraint\n",
      "Scaling Up Stochastic Dual Coordinate Ascent Kenneth Tran\n",
      "Heterogeneous Network Embedding via Deep Architectures\n",
      "Discovering Valuable Items from Massive Data Hastagiri P Vanchinathan\n",
      "Deep Learning Architecture with Dynamically Programmed Layers for Brain Connectome Prediction\n",
      "Incorporating World Knowledge to Document Clustering via Heterogeneous Information Networks\n"
     ]
    }
   ],
   "source": [
    "# Using metacharacters vice literal matches\n",
    "p=re.compile('^(.*)([\\s]){2}[A-z]+[\\s]+[\\s]?.+')\n",
    "\n",
    "for fileid in kddcorpus.fileids()[:25]:\n",
    "    print re.search('^(.*)[\\s]+[\\s]?(.*)?',kddcorpus.raw(fileid)).group(1).strip()+\" \"+re.search('^(.*)[\\s]+[\\s]?(.*)?',kddcorpus.raw(fileid)).group(2).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>This code extracts the titles, but some author names get caught up in the extraction as well. \n",
    "\n",
    "For simplicity, let's focus on wrangling the data to use the NERC tools on two sections of the paper: the “top” section and the “references” section.  The “top” section includes the names of authors and schools.  This section represents all of the text above the article’s abstract.  The “references” section appears at the end of the article. The regex tools of choice to extract sections are the [`positive lookbehind` and `positive lookahead`](https://docs.python.org/2/library/re.html) expressions.  We build two functions designed to extract the “top” and “references” sections of each document. \n",
    "\n",
    "First a few words about the data.  When working with natural language, one should always be prepared to deal with irregularities in the data set. This corpus is no exception. It comes from a top-notch data mining organization, but human error and a lack of standardization makes its way into the picture.  For example, in one paper the header section is entitled “Categories and Subject Descriptors,” while in another the title is “Categories & Subject Descriptors.” While that may seem like a small difference, these types of differences cause significant problems.  There are also some documents that will be missing sections altogether, i.e. keynote speaker documents do not contain a “references” section. When encountering similar issues in your work, you must decide whether to account for these differences or ignore them. I worked to include as much of the 253-document corpus as possible. \n",
    "\n",
    "In addition to extracting the relevant sections of the documents, our two functions will obtain a character count for each section, extract emails, count the number of references and store that value, calculate a word per reference count, and store all the above data as a nested dictionary with filenames as the key. For simplicity, we show below the code to extract the “references” section and include the function for extracting the “top” section in the Appendix. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Code to pull the references section only, store a character count, number of references, and \"word per reference\" calculation\n",
    "\n",
    "def refpull(docnum=None,section='references',full = False):\n",
    "    \n",
    "    # Establish an empty dictionary to hold values\n",
    "    ans={}\n",
    "    \n",
    "    # Establish an empty list to hold document ids that don't make the cut (i.e. missing reference section or different format)\n",
    "    # This comes in handy when you are trying to improve your code to catch outliers\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    \n",
    "    # Admin code to set default values and raise an exception if there's human error on input\n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "    \n",
    "    if docnum is None and full == True:\n",
    "        \n",
    "        # Setting the target document and the text we will extract from \n",
    "        text=kddcorpus.raw(docnum)\n",
    "        \n",
    "        \n",
    "        # This first condtional is for pulling the target section for ALL documents in the corpus\n",
    "        if full == True:\n",
    "            \n",
    "            # Iterate over the corpus to get the id; this is possible from loading our docs into a custom NLTK corpus\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid)\n",
    "                \n",
    "                # These lines of code build our regular expression.\n",
    "                # In the other functions for abstract or keywords, you see how I use this technique to create different regex arugments\n",
    "                if section == \"references\":\n",
    "                    section1=[\"REFERENCES\"] \n",
    "                    \n",
    "                    # Just in case, making sure our target string is empty before we pass data into it; just a check\n",
    "                    target = \"\"   \n",
    "\n",
    "                    #We now build our lists iteratively to build our regex\n",
    "                    for sect in section1:\n",
    "                        \n",
    "                        # We embed exceptions to remove the possibility of our code stopping; we pass failed passes into a list\n",
    "                        try:\n",
    "                            \n",
    "                            # our machine built regex\n",
    "                            part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                            p=re.compile(part1)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            \n",
    "                            # Conditoin to make sure we don't get any empty string\n",
    "                            if len(target) > 50:\n",
    "\n",
    "                                # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                                try:\n",
    "                                    refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                                except:\n",
    "                                    print \"This file does not appear to have a references section\"\n",
    "                                    pass\n",
    "                                \n",
    "                                #These are all our values; we build a nested dictonary and store the calculated values\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"references\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                ans[str(fileid)][\"refcount\"]= refnum\n",
    "                                ans[str(fileid)][\"wordperRef\"]=round(float(len(nltk.word_tokenize(text)))/float(refnum))\n",
    "                                #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                                break\n",
    "                            else:\n",
    "\n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "\n",
    "            return ans\n",
    "            return failids\n",
    "                              \n",
    "        # This is to perform the same operations on just one document; same functionality as above.\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum)\n",
    "        \n",
    "        if section == \"references\":\n",
    "            section1=[\"REFERENCES\"] \n",
    "            target = \"\"   \n",
    "            for sect in section1:\n",
    "                try:\n",
    "                    part1= \"(?<=\"+sect+\")(.+)\"\n",
    "                    p=re.compile(part1)\n",
    "                    target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                    if len(target) > 50:\n",
    "                        # calculate the number of references in a journal; finds digits between [] in references section only\n",
    "                        try:\n",
    "                            refnum = len(re.findall('\\[(\\d){1,3}\\]',target))+1\n",
    "                        except:\n",
    "                            print \"This file does not appear to have a references section\"\n",
    "                            pass\n",
    "                        ans[str(docnum)]={}\n",
    "                        ans[str(docnum)][\"references\"]=target.strip()\n",
    "                        ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                        ans[str(docnum)][\"refcount\"]= refnum\n",
    "                        ans[str(docnum)][\"wordperRef\"]=float(len(nltk.word_tokenize(text)))/float(refnum)\n",
    "\n",
    "\n",
    "                        #print [fileid,len(target),len(text), refnum, len(nltk.word_tokenize(text))/refnum]\n",
    "                        break\n",
    "                    else:\n",
    "\n",
    "                        pass\n",
    "                except AttributeError:\n",
    "                    failids.append(docnum)\n",
    "                    pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The above code also makes use of the `nltk.word_tokenize` tool to create the \"word per reference\" statistic (takes time to run). \n",
    "Let's test the “references” extraction function and look at the output by obtaining the first 10 entries of the dictionary created by the function.  This dictionary holds all the extracted data and various calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call our function, setting \"full=True\" extracts ALL references in corpus\n",
    "test = refpull(full=True)\n",
    "\n",
    "# To get a quick glimpse, I use the example from this page: http://stackoverflow.com/questions/7971618/python-return-first-n-keyvalue-pairs-from-dict\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "man = collections.OrderedDict(test)\n",
    "\n",
    "x = itertools.islice(man.items(), 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tabulate`](https://pypi.python.org/pypi/tabulate) module is a great tool to visualize descriptive outputs in table format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename      Character Count    Number of references    Words per Reference\n",
      "----------  -----------------  ----------------------  ---------------------\n",
      "p2277.txt                6295                      33                    326\n",
      "p835.txt                 5347                      38                    319\n",
      "p865.txt                 5269                      27                    399\n",
      "p2089.txt                8734                      45                    181\n",
      "p1759.txt                3677                      31                    405\n",
      "p29.txt                  5101                      40                    265\n",
      "p2227.txt               10345                      36                    332\n",
      "p2099.txt                3949                      28                    374\n",
      "p725.txt                 5771                      37                    304\n",
      "p2019.txt                9101                      60                    171\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# A quick list comprehension to follow the example on the tabulate pypi page\n",
    "table = [[key,value['charcount'],value['refcount'], value['wordperRef']] for key,value in x]\n",
    "\n",
    "# print the pretty table; we invoke the \"header\" argument and assign custom header!!!!\n",
    "print tabulate(table,headers=[\"filename\",\"Character Count\", \"Number of references\",\"Words per Reference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Open Source NERC Tools: NLTK, Stanford NER and Polyglot\n",
    "\n",
    "\n",
    "Now that we have a method to obtain the corpus from the “top” and “references” sections of each article in the dataset, we are ready to perform the named entity extractions.  In this post, we examine three popular, open source NERC tools. The tools are NLTK, Stanford NER, and Polyglot.  A brief description of each follows.\n",
    "\n",
    "[`NLTK has a chunk package`](http://www.nltk.org/api/nltk.chunk.html) that uses NLTK’s recommended named entity chunker to chunk the given list of tagged tokens. A string is tokenized and tagged with parts of speech (POS) tags.  The NLTK chunker then identifies non-overlapping groups and assigns them to an entity class. You can read more about NLTK's chunking capabilities in [the NLTK book](http://www.nltk.org/book/ch07.html).\n",
    "\n",
    "[`Standard's Named Entity Recognizer`](http://nlp.stanford.edu/software/CRF-NER.shtml), often called Stanford NER, is a Java implementation of linear chain Conditional Random Field (CRF) sequence models functioning as a Named Entity Recognizer. Named Entity Recognition (NER) labels sequences of words in a text that are the names of things, such as person and company names, or gene and protein names. NLTK contains an [interface to Stanford NER](http://www.nltk.org/_modules/nltk/tag/stanford.html) written by Nitin Madnani. Details for [using the Stanford NER tool](http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford) are on the NLTK page and the required jar files can be downloaded [here](http://nlp.stanford.edu/software/index.shtml).\n",
    "\n",
    "[`Polyglot`](http://polyglot.readthedocs.org/en/latest/index.html) is a natural language pipeline that supports massive multilingual (i.e. language) applications. It supports tokenization in 165 languages, language detection in 196 languages, named entity recognition in 40 languages, part of speech tagging in 16 languages, sentiment analysis in 136 languages, word embeddings in 137 languages, morphological analysis in 135 languages, and transliteration in 69 languages. It is a powerhouse tool for natural language processing. We will use the named entity recognition feature for English language in this exercise. Polyglot is available via pypi.\n",
    "\n",
    "\n",
    "We can now test how well these open source NERC tools extract entities from the “top” and “reference” sections of our corpus. For two documents, I hand labeled authors, organizations, and locations from the “top” section of the article (section before the abstract) and the list of all authors from the “references” section.  I also created a combined list of the authors, joining the lists from the “top” and “references” sections. Hand labeling is a time consuming and tedious process. For just the two (2) documents, this involved 295 cut-and-pastes of names or organizations. The annotated list appears in the Appendix.\n",
    "\n",
    "An easy test for the accuracy of a NERC tool is to compare the entities extracted by the tools to the hand-labeled extractions.  Before beginning, we take advantage of the NLTK functionality to obtain the “top” and “references” sections of the two documents used for the hand labeling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need the top and references sections from p19.txt and p29.txt\n",
    "\n",
    "p19={'top': toppull(\"p19.txt\")['p19.txt']['top'], 'references':refpull(\"p19.txt\")['p19.txt']['references']}\n",
    "p29={'top': toppull(\"p29.txt\")['p29.txt']['top'], 'references':refpull(\"p29.txt\")['p29.txt']['references']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>In this next block of code, we will apply the NLTK standard chunker, Stanford Named Entity Recognizer, and Polyglot extractor to our corpus. For each NERC tool, I created functions (available in the Appendix) to extract entities and return classes of objects in different lists. If you are following along, you should have run all the code blocks in the Appendix. If not, go there and do it now. The functions (in appendix) are:\n",
    "\n",
    "*  **nltktreelist** - NLTK Standard Chunker\n",
    "*  **get_continuous_chunks** - Stanford Named Entity Recognizer\n",
    "*  **extraction** - Polyglot Extraction tool\n",
    "\n",
    "For illustration, the Polyglot Extraction tool function, extraction, appears below:<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extraction(corpus):\n",
    "    import itertools\n",
    "    import unicodedata\n",
    "    from polyglot.text import Text\n",
    "    \n",
    "    corpus=corpus\n",
    "    # extract entities from a single string; remove whitespace characters\n",
    "    try:\n",
    "        e = Text(corpus).entities\n",
    "    except:\n",
    "        pass #e = Text(re.sub(\"(r'(x0)',\" \",\"(re.sub('[\\s]',\" \",corpus)))).entities\n",
    "    \n",
    "    current_person =[]\n",
    "    persons =[]\n",
    "    current_org=[]\n",
    "    organizations=[]\n",
    "    current_loc=[]\n",
    "    locations=[]\n",
    "\n",
    "    for l in e:\n",
    "        if l.tag == 'I-PER':\n",
    "            for m in l:\n",
    "                current_person.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_person: # if the current chunk is not empty\n",
    "                        persons.append(\" \".join(current_person))\n",
    "                        current_person = []\n",
    "        elif l.tag == 'I-ORG':\n",
    "            for m in l:\n",
    "                current_org.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_org: # if the current chunk is not empty\n",
    "                        organizations.append(\" \".join(current_org))\n",
    "                        current_org = []\n",
    "        elif l.tag == 'I-LOC':\n",
    "            for m in l:\n",
    "                current_loc.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_loc: # if the current chunk is not empty\n",
    "                        locations.append(\" \".join(current_loc))\n",
    "                        current_loc = []\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>We pass our data, the “top” and “references” section of the two documents of interest, into the functions created with each NERC tool and build a nested dictionary of the extracted entities—author names, locations, and organization names. This code may take a bit of time to run (30 secs to a minute). <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "    #  NLTK Standard Chunker\n",
    "###############################################\n",
    "    \n",
    "nltkstandard_p19ents = {'top': nltktreelist(p19['top']),'references': nltktreelist(p19['references'])}\n",
    "nltkstandard_p29ents = {'top': nltktreelist(p29['top']),'references': nltktreelist(p29['references'])}\n",
    "\n",
    "###############################################\n",
    "# Stanford NERC Tool\n",
    "################################################\n",
    "from nltk.tag import StanfordNERTagger, StanfordPOSTagger\n",
    "stner = StanfordNERTagger('/Users/linwood/stanford-corenlp-full/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "       '/Users/linwood/stanford-corenlp-full/stanford-corenlp-3.5.2.jar',\n",
    "       encoding='utf-8')\n",
    "stpos = StanfordPOSTagger('/Users/linwood/stanford-postagger-full/models/english-bidirectional-distsim.tagger','/Users/linwood/stanford-postagger-full/stanford-postagger.jar') \n",
    "\n",
    "stan_p19ents = {'top': get_continuous_chunks(p19['top']), 'references': get_continuous_chunks(p19['references'])}\n",
    "stan_p29ents = {'top': get_continuous_chunks(p29['top']), 'references': get_continuous_chunks(p29['references'])}\n",
    "\n",
    "###############################################\n",
    "# Polyglot NERC Tool\n",
    "###############################################\n",
    "poly_p19ents = {'top': extraction(p19['top']), 'references': extraction(p19['references'])}\n",
    "poly_p29ents = {'top': extraction(p29['top']), 'references': extraction(p29['references'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We will focus specifically on the \"persons\" entity extractions from the “top” section of the documents to estimate performance.  However, a similar exercise is possible with the extractions of “organizations” entity extractions or “locations” entity extractions too, as well as from the “references” section. To get a better look at how each NERC tool performed on the named person entities, we will use the `Pandas` dataframe.[`Pandas`](http://pandas.pydata.org/) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. The dataframe provides a visual comparison of the extractions from each NERC tool and the hand-labeled extractions. Just a few lines of code accomplish the task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hand-labeled True Authors</th>\n",
       "      <th>NLTKStandard NERC Authors</th>\n",
       "      <th>Stanford NERC Authors</th>\n",
       "      <th>Polyglot NERC Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Timeline</td>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Tim Althoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Safa Alai</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td></td>\n",
       "      <td>Safa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Van Dang</td>\n",
       "      <td>Safa Alai</td>\n",
       "      <td></td>\n",
       "      <td>Van Dang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wei Zhang</td>\n",
       "      <td>Van Dang</td>\n",
       "      <td></td>\n",
       "      <td>Wei Zhang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>Wei Zhang</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>Stanford</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>Mountain View</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hand-labeled True Authors NLTKStandard NERC Authors Stanford NERC Authors  \\\n",
       "0               Tim Althoff                  Timeline           Tim Althoff   \n",
       "1             Xin Luna Dong               Tim Althoff         Xin Luna Dong   \n",
       "2              Kevin Murphy             Xin Luna Dong          Kevin Murphy   \n",
       "3                 Safa Alai              Kevin Murphy                         \n",
       "4                  Van Dang                 Safa Alai                         \n",
       "5                 Wei Zhang                  Van Dang                         \n",
       "6                                           Wei Zhang                         \n",
       "7                                            Stanford                         \n",
       "8                                       Mountain View                         \n",
       "\n",
       "  Polyglot NERC Authors  \n",
       "0           Tim Althoff  \n",
       "1         Xin Luna Dong  \n",
       "2          Kevin Murphy  \n",
       "3                  Safa  \n",
       "4              Van Dang  \n",
       "5             Wei Zhang  \n",
       "6                        \n",
       "7                        \n",
       "8                        "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################\n",
    "# Administrative code, importing necessary library or module\n",
    "#################################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#################################################################\n",
    "# Create pandas series for each NERC tool entity extraction group\n",
    "#################################################################\n",
    "\n",
    "df1 = pd.Series(poly_p19ents['top']['persons'], index=None, dtype=None, name='Polyglot NERC Authors', copy=False, fastpath=False)\n",
    "df2=pd.Series(stan_p19ents['top']['persons'], index=None, dtype=None, name='Stanford NERC Authors', copy=False, fastpath=False)\n",
    "df3=pd.Series(nltkstandard_p19ents['top']['persons'], index=None, dtype=None, name='NLTKStandard NERC Authors', copy=False, fastpath=False)\n",
    "df4 = pd.Series(p19pdf_authors, index=None, dtype=None, name='Hand-labeled True Authors', copy=False, fastpath=False)\n",
    "met = pd.concat([df4,df3,df2,df1], axis=1).fillna('')\n",
    "met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The above dataframe illustrates the mixed results from the NERC tools.  NLTK Standard NERC appears to have extracted 3 false positives while the Stanford NERC missed 3 true positives and the Polyglot NERC extracted all but one true positive (partially extracted; returned first name only). Let's calculate some key performance metrics:<br>\n",
    "1.  **TN or True Negative**: case was negative and predicted negative <br>\n",
    "2.  **TP or True Positive**: case was positive and predicted positive <br>\n",
    "3.  **FN or False Negative**: case was positive but predicted negative <br>\n",
    "4.  **FP or False Positive**: case was negative but predicted positive<br>\n",
    "\n",
    "The following function calculates the above metrics for the three NERC tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculations and logic from http://www.kdnuggets.com/faq/precision-recall.html\n",
    "\n",
    "def metrics(truth,run):\n",
    "    truth = truth\n",
    "    run = run\n",
    "    TP = float(len(set(run) & set(truth)))\n",
    "\n",
    "    if float(len(run)) >= float(TP):\n",
    "        FP = len(run) - TP\n",
    "    else:\n",
    "        FP = TP - len(run)\n",
    "    TN = 0\n",
    "    if len(truth) >= len(run):\n",
    "        FN = len(truth) - len(run)\n",
    "    else:\n",
    "        FN = 0\n",
    "\n",
    "    accuracy = (float(TP)+float(TN))/float(len(truth))\n",
    "    recall = (float(TP))/float(len(truth))\n",
    "    precision = float(TP)/(float(FP)+float(TP))\n",
    "    print \"The accuracy is %r\" % accuracy\n",
    "    print \"The recall is %r\" % recall\n",
    "    print \"The precision is %r\" % precision\n",
    "    \n",
    "    d = {'Predicted Negative': [TN,FN], 'Predicted Positive': [FP,TP]}\n",
    "    metricsdf = pd.DataFrame(d, index=['Negative Cases','Positive Cases'])\n",
    "    \n",
    "    return metricsdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Now let's pass our values into the function to calculate the performance metrics:<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    NLTK Standard NERC Tool Metrics     \n",
      "\n",
      "\n",
      "The accuracy is 1.0\n",
      "The recall is 1.0\n",
      "The precision is 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                   3\n",
       "Positive Cases                   0                   6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print\n",
    "print\n",
    "str1 = \"NLTK Standard NERC Tool Metrics\"\n",
    "\n",
    "print str1.center(40, ' ')\n",
    "print\n",
    "print\n",
    "metrics(p19pdf_authors,nltkstandard_p19ents['top']['persons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "       Stanford NERC Tool Metrics       \n",
      "\n",
      "\n",
      "The accuracy is 0.5\n",
      "The recall is 0.5\n",
      "The precision is 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                   0\n",
       "Positive Cases                   3                   3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print\n",
    "print\n",
    "str2 = \"Stanford NERC Tool Metrics\"\n",
    "\n",
    "print str2.center(40, ' ')\n",
    "print\n",
    "print\n",
    "metrics(p19pdf_authors, stan_p19ents['top']['persons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "       Polyglot NERC Tool Metrics       \n",
      "\n",
      "\n",
      "The accuracy is 0.8333333333333334\n",
      "The recall is 0.8333333333333334\n",
      "The precision is 0.8333333333333334\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                   1\n",
       "Positive Cases                   0                   5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print\n",
    "print\n",
    "str3 = \"Polyglot NERC Tool Metrics\"\n",
    "\n",
    "print str3.center(40, ' ')\n",
    "print\n",
    "print\n",
    "metrics(p19pdf_authors,poly_p19ents['top']['persons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to Ben from Selma** - *I think there might be a mistake in the table for the Polyglot NERC. Missing a 1 in the lower left maybe?* \n",
    "\n",
    "The basic metrics above reveal some quick takeaways about each tool based on the specific extraction task. The `NLTK Standard Chunker` has perfect accuracy and recall but lacks in precision. It successfully extracted all the authors for the document, but also extracted 3 false entities. NLTK's chunker would serve well in an entity extraction pipeline where the data scientist is concerned with identifying all possible entities\n",
    "\n",
    "The `Stanford NER tool` is very precise (specificity vs sensitivity). The entities it extracts were 100% accurate, but it failed to identify half of the true entities. The Stanford NER tool would be best used when a data scientist wanted to extract only those entities that have a high likelihood of being named entities, suggesting an unconscious acceptance of leaving behind some information.\n",
    "\n",
    "The `Polyglot Named Entity Recognizer` identified five named entities exactly, but only partially identified the sixth (first name returned only). The data scientist looking for a balance between sensitivity and specificity would likely use Polyglot, as it will balance extracting the 100% accurate entities and those which may not necessarily be a named entity.\n",
    "\n",
    "### A Simple Ensemble Classifier\n",
    "\n",
    "In our discussion above, we notice the varying levels of performance by the different NERC tools. Using the idea that combining the outputs from various classifiers in an ensemble method can improve the reliability of classifications, we can improve the performance of our named entity extractor tools by creating an ensemble classifier. Each NERC tool had at least 3 named persons that were true positives, but no two NERC tools had the same false positive or false negative. Our ensemble classifier \"voting\" rule is very simple: “Return all named entities that exist in at least two of the true positive named entity result sets from our NERC tools.\n",
    "We implement this rule using the `set` module. We first do an `intersection` operation of the NERC results vs the hand labeled entities to get our \"true positive\" set. Here is our code to accomplish the task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Kevin Murphy',\n",
       " 'Safa Alai',\n",
       " 'Tim Althoff',\n",
       " 'Van Dang',\n",
       " 'Wei Zhang',\n",
       " 'Xin Luna Dong'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create intersection of true authors from NLTK standard output\n",
    "a =set(sorted(nltkstandard_p19ents['top']['persons'])) & set(p19pdf_authors)\n",
    "\n",
    "# Create intersection of true authors from Stanford NER output\n",
    "b =set(sorted(stan_p19ents['top']['persons'])) & set(p19pdf_authors)\n",
    "\n",
    "# Create intersection of true authors from Polyglot output\n",
    "c = set(sorted(poly_p19ents['top']['persons'])) & set(p19pdf_authors)\n",
    "\n",
    "# Create union of all true positives from each NERC output\n",
    "(a.union(b)).union(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a visual comparison of the extractions for each tool and the ensemble set side by side, we return to our dataframe from earlier. In this case, we use the `concat` operation in `pandas` to append the new ensemble set to the dataframe. Our code to accomplish the task is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hand-labeled True Authors</th>\n",
       "      <th>Ensemble Method Authors</th>\n",
       "      <th>NLTKStandard NERC Authors</th>\n",
       "      <th>Stanford NERC Authors</th>\n",
       "      <th>Polyglot NERC Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Wei Zhang</td>\n",
       "      <td>Timeline</td>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Tim Althoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Tim Althoff</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Xin Luna Dong</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Safa Alai</td>\n",
       "      <td>Van Dang</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td></td>\n",
       "      <td>Safa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Van Dang</td>\n",
       "      <td>Kevin Murphy</td>\n",
       "      <td>Safa Alai</td>\n",
       "      <td></td>\n",
       "      <td>Van Dang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wei Zhang</td>\n",
       "      <td>Safa Alai</td>\n",
       "      <td>Van Dang</td>\n",
       "      <td></td>\n",
       "      <td>Wei Zhang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wei Zhang</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Stanford</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Mountain View</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hand-labeled True Authors Ensemble Method Authors NLTKStandard NERC Authors  \\\n",
       "0               Tim Althoff               Wei Zhang                  Timeline   \n",
       "1             Xin Luna Dong             Tim Althoff               Tim Althoff   \n",
       "2              Kevin Murphy           Xin Luna Dong             Xin Luna Dong   \n",
       "3                 Safa Alai                Van Dang              Kevin Murphy   \n",
       "4                  Van Dang            Kevin Murphy                 Safa Alai   \n",
       "5                 Wei Zhang               Safa Alai                  Van Dang   \n",
       "6                                                                   Wei Zhang   \n",
       "7                                                                    Stanford   \n",
       "8                                                               Mountain View   \n",
       "\n",
       "  Stanford NERC Authors Polyglot NERC Authors  \n",
       "0           Tim Althoff           Tim Althoff  \n",
       "1         Xin Luna Dong         Xin Luna Dong  \n",
       "2          Kevin Murphy          Kevin Murphy  \n",
       "3                                        Safa  \n",
       "4                                    Van Dang  \n",
       "5                                   Wei Zhang  \n",
       "6                                              \n",
       "7                                              \n",
       "8                                              "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfensemble = pd.Series(list((a.union(b)).union(c)), index=None, dtype=None, name='Ensemble Method Authors', copy=False, fastpath=False)\n",
    "met = pd.concat([df4,dfensemble,df3,df2,df1], axis=1).fillna('')\n",
    "met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get a look at the performance metrics to see if we push our scores up in all categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "         Ensemble NERC Metrics          \n",
      "\n",
      "\n",
      "The accuracy is 1.0\n",
      "The recall is 1.0\n",
      "The precision is 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive Cases</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Predicted Negative  Predicted Positive\n",
       "Negative Cases                   0                   0\n",
       "Positive Cases                   0                   6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print\n",
    "print\n",
    "str = \"Ensemble NERC Metrics\"\n",
    "\n",
    "print str.center(40, ' ')\n",
    "print\n",
    "print\n",
    "metrics(p19pdf_authors,list((a.union(b)).union(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>Exactly as expected, we see improved performance across all performance metric scores and in the end get a perfect extraction of all named persons from this document. Before we go ANY further, the idea of moving from \"okay\" to \"perfect\" is unrealistic. Moreover, this is a very small sample and only intended to show the application of an ensemble method. Applying this method to other sections of the journal articles will not lead to a perfect extraction, but it will indeed improve the performance of the extraction considerably.\n",
    "\n",
    "### Getting Your Data in Open File Format\n",
    "\n",
    "A good rule for any data analytics project is to store the results or output in an open file format. Why? An [open file format is a published specification for storing digital data, usually maintained by a standards organization, and which can be used and implemented by anyone](https://en.wikipedia.org/wiki/Open_format).  I selected [`JavaScript Object Notation(JSON)`](https://en.wikipedia.org/wiki/JSON), which is an open standard format that uses human-readable text to transmit data objects consisting of attribute–value pairs. We take our list of persons from the ensemble results, store it as a Python dictionary, and then convert it to JSON. Alternatively, we could use the `dumps` function from the `json` module to return dictionaries, and ensure we get the open file format at every step. \n",
    "\n",
    "In this way, other data scientists or users could pick and choose what portions of code to use in their projects. Here is our code to accomplish the task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   \"Wei Zhang\", \n",
      "   \"Tim Althoff\", \n",
      "   \"Xin Luna Dong\", \n",
      "   \"Van Dang\", \n",
      "   \"Kevin Murphy\", \n",
      "   \"Safa Alai\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Add ensemble results for author to the nested python dictionary\n",
    "p19['authors']= list((a.union(b)).union(c))\n",
    "\n",
    "# covert nested dictionary to json for open data storage\n",
    "# json can be stored in mongodb or any other disk store\n",
    "output = json.dumps(p19, ensure_ascii=False,indent=3)\n",
    "\n",
    "# print out the authors section we just created in our json\n",
    "print json.dumps(json.loads(output)['authors'],indent=3)\n",
    "\n",
    "# uncomment to see full json output\n",
    "#print json.dumps((json.loads(output)),indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We covered the entire data science pipeline in a natural language processing job that compared the performance of three different NERC tools. A core task in this pipeline involved ingesting plaintext into an NLTK corpus so that we could easily retrieve and manipulate the corpus. Finally, we used the results from the various NERC tools to create a simplistic ensemble classifier that improved the overall performance.\n",
    "\n",
    "The techniques in this post can be applied to other domains, larger datasets or any other corpus. Everything I used in this post (with the exception of the Regular expression resource from Coursera) was not taught in a classroom or structured learning experience. It all came from online resources, posts from others, and books (that includes learning how to code in Python). If you have the motivation, you can do it. \n",
    "\n",
    "Throughout the article, there are hyperlinks to resources and reading materials for reference, but here is a central list:\n",
    "\n",
    "* [Requirements to run this code in iPython notebook or on your machine](https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/namedentity_requirements.yml)\n",
    "* [Natural Language Toolkit Book (free online resource)](http://www.nltk.org/book/) and the [NLTK Standard Chunker](http://www.nltk.org/_modules/nltk/chunk/named_entity.html) and a [post on how to use the chunker](http://stackoverflow.com/questions/19312573/nltk-for-named-entity-recognition)\n",
    "* [Polyglot natural language pipeline for massive muliligual applications](https://pypi.python.org/pypi/polyglot) and the [journal article describing the word classification model](http://arxiv.org/pdf/1410.3791.pdf)\n",
    "* [Stanford Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml) and the [NLTK interface to the Stanford NER](http://www.nltk.org/_modules/nltk/tag/stanford.html) and a [post on how to use the interface](http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages)\n",
    "* [Python Pandas](http://pandas.pydata.org/) is a must have tool for anyone who does analysis in Python.  The best book I've used to date is [Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython](https://play.google.com/store/books/details?id=v3n4_AK8vu0C&source=productsearch&utm_source=HA_Desktop_US&utm_medium=SEM&utm_campaign=PLA&pcampaignid=MKTAD0930BO1&gl=US&gclid=COnf8Z_BncoCFYKvNwodVA4ItA&gclsrc=ds)\n",
    "* [Intuitive description and examples of Python's standard library set module](http://www.linuxtopia.org/online_books/programming_books/python_programming/python_ch16s03.html)\n",
    "* [Discussion of ensemble classifiers](http://arxiv.org/pdf/1404.4088.pdf)\n",
    "* [Nice module to print tables in standard python output called tablulate](https://pypi.python.org/pypi/tabulate)\n",
    "* [Regular expression training (more examples in earlier sections)](http://regexone.com/)\n",
    "* [Python library to extract text from PDF](http://euske.github.io/pdfminer/index.html) and [post on available Python tools to extract text from a PDF](https://www.binpress.com/tutorial/manipulating-pdfs-with-python/167)\n",
    "* [ACM Digital Library](http://dl.acm.org/) to [purchase journal articles to completely recreate this exercise](https://dl.acm.org/purchase.cfm?id=2783258&CFID=740512201&CFTOKEN=34489585)\n",
    "* My [quick web scrap code to pull back abstracts and authors from KDD 2015](https://github.com/linwoodc3/LC3-Creations/blob/master/DDL/namedentityblog/KDDwebscrape.ipynb); can apply this same analysis to web acquired dataset\n",
    "\n",
    "If you liked this post, make sure to go to the [blog home page](http://districtdatalabs.silvrback.com/) and click the **Subscribe** button so that you don't miss any of our future posts. We're also always looking for blog contributors, so if you have data science skills and want to get some exposure, [apply here](http://www.districtdatalabs.com/#!blog-contributor/c4m8).\n",
    "\n",
    "### References\n",
    "\n",
    "<sup id=\"fn1\">1. [(2014). Text Mining and its Business Applications - CodeProject. Retrieved December 26, 2015, from http://www.codeproject.com/Articles/822379/Text-Mining-and-its-Business-Applications.]<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\">↩</a></sup>\n",
    "\n",
    "<sup id=\"fn2\">2. [Suchanek, F., & Weikum, G. (2013). Knowledge harvesting in the big-data era. Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM.]<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\">↩</a></sup>\n",
    "\n",
    "\n",
    "<sup id =\"fn3\">3. [Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1), 3-26.]<a href=\"#ref3\" title = \"Jump back to footnote 3 in the text\">↩</a></sup>\n",
    "\n",
    "<sup id =\"fn4\">4. [Ojeda, Tony, Sean Patrick Murphy, Benjamin Bengfort, and Abhijit Dasgupta. [Practical Data Science Cookbook: 89 Hands-on Recipes to Help You Complete Real-world Data Science Projects in R and Python](https://www.packtpub.com/big-data-and-business-intelligence/practical-data-science-cookbook). N.p.: n.p., n.d. Print.]<a href=\"#ref4\" title = \"Jump back to footnote 4 in the text\">↩</a></sup>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "Create all the functions in the appendix before running this code in a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand labeled entities from two journal articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "p19pdf_authors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','1600 Amphitheatre Parkway, Mountain View, CA 94043','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "p19pdf_allauthors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang','A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pull from top section of document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attempting function with gold top section...Normal case done\n",
    "\n",
    "def toppull(docnum=None,section='top',full = False):\n",
    "    from emailextractor import file_to_str, get_emails # paste code to .py file from following link and save within your environment path to call it: https://gist.github.com/dideler/5219706\n",
    "\n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "\n",
    "    if docnum is None and full == True:\n",
    "\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "\n",
    "        if full == True:\n",
    "            if section == 'top':\n",
    "                section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "                for fileid in kddcorpus.fileids():\n",
    "                    text = kddcorpus.raw(fileid)\n",
    "                    for sect in section:\n",
    "                        try:\n",
    "                            part1=\"(.+)(?=\"+sect+\")\"\n",
    "                            #print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                            p=re.compile(part1)\n",
    "                            target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                            #print docnum,len(target),len(text)\n",
    "\n",
    "                            emails = tuple(get_emails(target))\n",
    "                            ans[str(fileid)]={}\n",
    "                            ans[str(fileid)][\"top\"]=target.strip()\n",
    "                            ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                            ans[str(fileid)][\"emails\"]=emails\n",
    "                            #print [fileid,len(target),len(text)]\n",
    "                            break\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "        return ans\n",
    "        return failids\n",
    "\n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum)\n",
    "\n",
    "        if section == \"top\":\n",
    "            section = [\"ABSTRACT\",\"Abstract\",\"Bio\",\"Panel Summary\"]\n",
    "            text = kddcorpus.raw(docnum)\n",
    "            for sect in section:\n",
    "                try:\n",
    "                    part1=\"(.+)(?=\"+sect+\")\"\n",
    "                    #print \"re.compile\"+\"(\"+part1+\")\"\n",
    "                    p=re.compile(part1)\n",
    "                    target = p.search(re.sub('[\\s]',\" \", text)).group()\n",
    "                    #print docnum,len(target),len(text)\n",
    "\n",
    "                    emails = tuple(get_emails(target))\n",
    "                    ans[str(docnum)]={}\n",
    "                    ans[str(docnum)][\"top\"]=target.strip()\n",
    "                    ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                    ans[str(docnum)][\"emails\"]=emails\n",
    "                    #print [fileid,len(target),len(text)]\n",
    "                    break\n",
    "\n",
    "                except AttributeError:\n",
    "                    failids.append(fileid)\n",
    "                    pass\n",
    "\n",
    "        return ans\n",
    "        return failids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to build list of named entity classes from Standard NLTK Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltktreelist(text):\n",
    "    from operator import itemgetter\n",
    "\n",
    "    text = text\n",
    "\n",
    "\n",
    "    persons = []\n",
    "    organizations = []\n",
    "    locations =[]\n",
    "    genpurp = []\n",
    "\n",
    "    for l in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(l,nltk.tree.Tree):\n",
    "            if l.label() == 'PERSON':\n",
    "                if len(l)== 1:\n",
    "                    if l[0][0] in persons:\n",
    "                        pass\n",
    "                    else:\n",
    "                        persons.append(l[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), l)) in persons:\n",
    "                        pass\n",
    "                    else:\n",
    "                        persons.append(\" \".join(map(itemgetter(0), l)).strip(\"*\"))\n",
    "\n",
    "\n",
    "    for o in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'ORGANIZATION':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in organizations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        organizations.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in organizations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        organizations.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "\n",
    "\n",
    "    for o in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'LOCATION':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in locations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        locations.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in locations:\n",
    "                        pass\n",
    "                    else:\n",
    "                        locations.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "\n",
    "    for e in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "        if isinstance(o,nltk.tree.Tree):\n",
    "            if o.label() == 'GPE':\n",
    "                if len(o)== 1:\n",
    "                    if o[0][0] in genpurp:\n",
    "                        pass\n",
    "                    else:\n",
    "                        genpurp.append(o[0][0])\n",
    "                else:\n",
    "                    if \" \".join(map(itemgetter(0), o)) in genpurp:\n",
    "                        pass\n",
    "                    else:\n",
    "                        genpurp.append(\" \".join(map(itemgetter(0), o)).strip(\"*\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    results['genpurp'] = genpurp\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get lists of entities from Stanford NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(string):\n",
    "    string = string\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in stner.tag(string.split()):\n",
    "        if tag != \"O\":\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk: # if the current chunk is not empty\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    named_entities = continuous_chunk\n",
    "    named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "    named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n",
    "    persons = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"PERSON\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    persons.append(n.strip(\"*\"))\n",
    "    organizations = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"ORGANIZATION\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                n.strip(\"*\")\n",
    "                if len(n)>0:\n",
    "                    organizations.append(n.strip(\"*\"))\n",
    "    locations = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"LOCATION\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    locations.append(n.strip(\"*\"))\n",
    "    dates = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"DATE\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    dates.append(n.strip(\"*\"))\n",
    "    money = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"MONEY\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "    time = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"TIME\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "\n",
    "    percent = []\n",
    "    for l in [l.split(\",\") for l,m in named_entities_str_tag if m == \"PERCENT\"]:\n",
    "        for m in l:\n",
    "            for n in m.strip().split(\",\"):\n",
    "                if len(n)>0:\n",
    "                    money.append(n.strip(\"*\"))\n",
    "\n",
    "    entities={}\n",
    "    entities['persons']= persons\n",
    "    entities['organizations']= organizations\n",
    "    entities['locations']= locations\n",
    "    #entities['dates']= dates\n",
    "    #entities['money']= money\n",
    "    #entities['time']= time\n",
    "    #entities['percent']= percent\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pull Keywords section only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# attempting function with gold keywords....\n",
    "\n",
    "def keypull(docnum=None,section='keywords',full = False):\n",
    "\n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "\n",
    "    if docnum is None and full == True:\n",
    "\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "\n",
    "\n",
    "\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid).lower()\n",
    "                if section == \"keywords\":\n",
    "                    section1=\"keywords\"\n",
    "                    target = \"\"   \n",
    "                    section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"(1. tutorial )\",\" permission to make \",\"  permission to make\",\"(  permission to make digital )\",\"    bio  \",\"abstract:  \",\"1.motivation\" ]\n",
    "\n",
    "                    part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "                    for sect in section2:\n",
    "                        try:\n",
    "                            part2 = \"(?=\"+str(sect)+\")\"\n",
    "                            p=re.compile(part1+part2)\n",
    "                            target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                            if len(target) >50:\n",
    "                                if len(target) > 300:\n",
    "                                    target = target[:200]\n",
    "                                else:\n",
    "                                    target = target\n",
    "\n",
    "                                ans[str(fileid)]={}\n",
    "                                ans[str(fileid)][\"keywords\"]=target.strip()\n",
    "                                ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                #print [fileid,len(target),len(text)]\n",
    "                                break\n",
    "                            else:\n",
    "                                if len(target)==0:\n",
    "                                     failids.append(fileid)   \n",
    "                                pass\n",
    "                        except AttributeError:\n",
    "                            failids.append(fileid)\n",
    "                            pass\n",
    "            set(failids)\n",
    "            return ans\n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        if full == False:\n",
    "            if section == \"keywords\":\n",
    "                section1=\"keywords\"\n",
    "                target = \"\"   \n",
    "                section2=[\"1.  introduction  \",\"1.  introd \",\"1. motivation\",\"permission to make \",\"1.motivation\" ]\n",
    "\n",
    "                part1= \"(?<=\"+str(section1)+\")(.+)\"\n",
    "\n",
    "                for sect in section2:\n",
    "                    try:\n",
    "                        part2 = \"(?=\"+str(sect)+\")\"\n",
    "                        p=re.compile(part1+part2)\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group(1)\n",
    "                        if len(target) >50:\n",
    "                            if len(target) > 300:\n",
    "                                target = target[:200]\n",
    "                            else:\n",
    "                                target = target\n",
    "                            ans[docnum]={}\n",
    "                            ans[docnum][\"keywords\"]=target.strip()\n",
    "                            ans[docnum][\"charcount\"]=len(target)\n",
    "                            break                  \n",
    "                    except:\n",
    "                        pass\n",
    "    return ans\n",
    "    return failids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pull Abstract only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attempting function with gold abstracts...Normal case done\n",
    "\n",
    "def abpull(docnum=None,section='abstract',full = False):\n",
    "\n",
    "    ans={}\n",
    "    failids = []\n",
    "    section = section.lower()    \n",
    "    if docnum is None and full == False:\n",
    "        raise BaseException(\"Enter target file to extract data from\")\n",
    "\n",
    "    if docnum is None and full == True:\n",
    "\n",
    "        text=kddcorpus.raw(docnum).lower()\n",
    "        # to return output from entire corpus\n",
    "        if full == True:\n",
    "            for fileid in kddcorpus.fileids():\n",
    "                text = kddcorpus.raw(fileid)\n",
    "                if section == \"abstract\":\n",
    "                    section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "                    target = \"\"   \n",
    "                    section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "                    for fileid in kddcorpus.fileids():\n",
    "                        text = kddcorpus.raw(fileid)\n",
    "\n",
    "\n",
    "                        for sect1 in section1:\n",
    "                            for sect2 in section2:\n",
    "                                part1= \"(?<=\"+str(sect1)+\")(.+)\"\n",
    "                                part2 = \"(?=\"+str(sect2)+\")\"\n",
    "                                p = re.compile(part1+part2)\n",
    "                                try:\n",
    "                                    target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                                    if len(target) > 50:\n",
    "                                        ans[str(fileid)]={}\n",
    "                                        ans[str(fileid)][\"abstract\"]=target.strip()\n",
    "                                        ans[str(fileid)][\"charcount\"]=len(target)\n",
    "                                        #print [fileid,len(target),len(text)]\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        failids.append(fileid)\n",
    "                                        pass\n",
    "                                except AttributeError:\n",
    "                                    pass \n",
    "\n",
    "            return ans\n",
    "\n",
    "        # to return output from one document\n",
    "    else:\n",
    "        ans = {}\n",
    "        failids=[]\n",
    "        text = kddcorpus.raw(docnum).lower()\n",
    "        if section == \"abstract\":\n",
    "            section1=[\"ABSTRACT\", \"Abstract \"]\n",
    "            target = \"\"   \n",
    "            section2=[\"Categories and Subject Descriptors\",\"Categories & Subject Descriptors\",\"Keywords\",\"INTRODUCTION\"]\n",
    "            for sect1 in section1:\n",
    "                for sect2 in section2:\n",
    "                    part1= \"(?<=\"+str(sect1)+\")(.+?)\"\n",
    "                    part2 = \"(?=\"+str(sect2)+\"[\\s]?)\"\n",
    "                    p = re.compile(part1+part2)\n",
    "                    try:\n",
    "                        target=p.search(re.sub('[\\s]',\" \",text)).group()\n",
    "                        if len(target) > 50:\n",
    "                            ans[str(docnum)]={}\n",
    "                            ans[str(docnum)][\"abstract\"]=target.strip()\n",
    "                            ans[str(docnum)][\"charcount\"]=len(target)\n",
    "                            #print [docnum,len(target),len(text)]\n",
    "                            break\n",
    "                        else:\n",
    "                            failids.append(docnum)\n",
    "                            pass\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "        return ans\n",
    "        return failids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py27)",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
